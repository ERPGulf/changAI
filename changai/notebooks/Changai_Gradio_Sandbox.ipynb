{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1zZ-QVhiWfkyqCFezrPW7nvP-TNUzl7nc",
      "authorship_tag": "ABX9TyNN0IEfSQxcCAfvr9vWHXyk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVAjXR4yDltA"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentence-transformers gradio\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import json\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    T5Tokenizer, T5ForConditionalGeneration\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer, util"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "dG6Qs1XxLTN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_fields_with_flan(doctype, question, top_fields):\n",
        "    prompt = f\"Instruction: Select only the correct field(s) from the given top fields that answer the question.\\nDoctype: {doctype}\\nQuestion: {question}\\nTop Fields: {', '.join(top_fields)}\"\n",
        "    input_ids = tokenizer_s2(prompt, return_tensors=\"pt\").input_ids\n",
        "    output_ids = model_s2.generate(input_ids, max_length=64)\n",
        "    return tokenizer_s2.decode(output_ids[0], skip_special_tokens=True).split(\", \")\n",
        "doctype=\"Journal Entry\"\n",
        "question=\"Fetch all journal entries created last month.\"\n",
        "top_fields=['posting_date', 'due_date', 'cheque_date', 'clearance_date']\n",
        "try:\n",
        "    selected_fields = select_fields_with_flan(doctype, question, top_fields)\n",
        "    print(\"‚úÖ Selected Fields (FLAN):\", selected_fields)\n",
        "except Exception as flan_err:\n",
        "    print(f\"‚ùå Error in select_fields_with_flan: {flan_err}\")\n",
        "    selected_fields = []"
      ],
      "metadata": {
        "id": "ylcMkZ3GKiOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Changai/S1/Datasets/id2label\") as f:\n",
        "    id2label = json.load(f)"
      ],
      "metadata": {
        "id": "eUsI9mOpMlbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage 1: Doctype Classifier (RoBERTa)\n",
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n",
        "s1_model = \"hyrinmansoor/text2frappe-s1-roberta\"\n",
        "tokenizer_s1 = RobertaTokenizerFast.from_pretrained(s1_model)\n",
        "model_s1 = RobertaForSequenceClassification.from_pretrained(s1_model)\n",
        "\n",
        "# Load label mapping from uploaded file\n",
        "with open(\"/content/drive/MyDrive/Changai/meta.json\") as f:\n",
        "    meta = json.load(f)\n",
        "# with open(\"/content/meta.json\") as f:\n",
        "#     s2_meta = json.load(f)\n",
        "\n",
        "sbert = SentenceTransformer(\"hyrinmansoor/text2frappe-s2-sbert\")\n",
        "\n",
        "s2_model = \"hyrinmansoor/text2frappe-s2-flan-field\"\n",
        "tokenizer_s2 = T5Tokenizer.from_pretrained(s2_model)\n",
        "model_s2 = T5ForConditionalGeneration.from_pretrained(s2_model)\n",
        "\n",
        "s3_model = \"hyrinmansoor/text2frappe-s3-flan-query\"\n",
        "tokenizer_s3 = T5Tokenizer.from_pretrained(s3_model)\n",
        "model_s3 = T5ForConditionalGeneration.from_pretrained(s3_model)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Changai/S1/Datasets/id2label\") as f:\n",
        "    id2label = json.load(f)\n",
        "\n",
        "def predict_doctype(question):\n",
        "    try:\n",
        "        inputs = tokenizer_s1(question, return_tensors=\"pt\", truncation=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model_s1(**inputs)\n",
        "        pred_id = str(outputs.logits.argmax().item())\n",
        "        print(\"üî¢ Predicted ID:\", pred_id)\n",
        "\n",
        "        predicted_doctype = id2label.get(pred_id, \"Unknown\")\n",
        "\n",
        "        print(\"üìÑ Predicted Doctype:\", predicted_doctype)\n",
        "        return predicted_doctype\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Exception in predict_doctype:\", str(e))\n",
        "        return \"Unknown\"\n",
        "\n",
        "def get_top_k_fields(question, doctype, k=4):\n",
        "    try:\n",
        "        if doctype not in meta or \"fields\" not in meta[doctype]:\n",
        "            raise ValueError(f\"‚ùå No metadata or fields found for doctype '{doctype}'\")\n",
        "\n",
        "        fields = meta[doctype][\"fields\"]\n",
        "        print(f\"üìã Fields for {doctype}: {fields}\")\n",
        "\n",
        "        if not fields:\n",
        "            print(f\"‚ö†Ô∏è Field list is empty for doctype: {doctype}\")\n",
        "            return []\n",
        "\n",
        "        sbert_prompt = f\"Doctype: {doctype}\\nQuestion: {question}\"\n",
        "\n",
        "        query_emb = sbert.encode(sbert_prompt, convert_to_tensor=True)\n",
        "        field_embs = sbert.encode(fields, convert_to_tensor=True)\n",
        "\n",
        "        sim_scores = util.pytorch_cos_sim(query_emb, field_embs)[0]\n",
        "        top_k = torch.topk(sim_scores, k=min(k, len(fields)))\n",
        "\n",
        "        return [fields[i] for i in top_k.indices.tolist()]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Exception in get_top_k_fields:\", str(e))\n",
        "        return []\n",
        "\n",
        "def select_fields_with_flan(doctype, question, top_fields):\n",
        "    prompt = f\"Instruction: Select only the correct field(s) from the given top fields that answer the question.\\nDoctype: {doctype}\\nQuestion: {question}\\nTop Fields: {', '.join(top_fields)}\"\n",
        "    input_ids = tokenizer_s2(prompt, return_tensors=\"pt\").input_ids\n",
        "    output_ids = model_s2.generate(input_ids, max_length=64)\n",
        "    return tokenizer_s2.decode(output_ids[0], skip_special_tokens=True).split(\", \")\n",
        "\n",
        "def generate_frappe_query(doctype, question, fields):\n",
        "    prompt = f\"Generate the correct Frappe query for the given question, using the provided doctype and fields.\\nDoctype: {doctype}\\nQuestion: {question}\\nFields: {', '.join(fields)}\"\n",
        "    input_ids = tokenizer_s3(prompt, return_tensors=\"pt\").input_ids\n",
        "    output_ids = model_s3.generate(input_ids, max_length=128)\n",
        "    decoded= tokenizer_s3.decode(output_ids[0], skip_special_tokens=False)\n",
        "    decoded=decoded.replace(\"<pad>\",\"\")\n",
        "    decoded=decoded.replace(\"</s>\",\"\")\n",
        "    return decoded\n",
        "\n",
        "def full_pipeline(question):\n",
        "    try:\n",
        "        print(\"\\nüìå Input Question:\", question)\n",
        "        doctype = predict_doctype(question)\n",
        "        print(\"‚úÖ Predicted Doctype:\", doctype)\n",
        "\n",
        "        try:\n",
        "            top_fields = get_top_k_fields(question, doctype)\n",
        "            print(\"üîé Top Fields (SBERT):\", top_fields)\n",
        "        except Exception as field_err:\n",
        "            print(f\"‚ùå Error in get_top_k_fields: {field_err}\")\n",
        "            top_fields = [{field_err}]\n",
        "        try:\n",
        "            selected_fields = select_fields_with_flan(doctype, question, top_fields)\n",
        "            print(\"‚úÖ Selected Fields (FLAN):\", selected_fields)\n",
        "        except Exception as flan_err:\n",
        "            print(f\"‚ùå Error in select_fields_with_flan: {flan_err}\")\n",
        "            selected_fields = []\n",
        "        try:\n",
        "            frappe_query = generate_frappe_query(doctype, question, selected_fields)\n",
        "            print(\"‚úÖ Frappe Query (FLAN):\", frappe_query)\n",
        "        except:\n",
        "            frappe_query = \"‚ùå Error in generate_frappe_query\"\n",
        "        return (\n",
        "            f\"üìÑ **Predicted Doctype**: {doctype}\",\n",
        "            f\"üîé **Top Fields (SBERT)**: {top_fields}\",\n",
        "            f\"‚úÖ **Selected Fields (FLAN)**: {selected_fields}\",\n",
        "            f\"üìå**Frappe Query (FLAN)**:{frappe_query}\"\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå ERROR in full_pipeline:\", str(e))\n",
        "        return (\n",
        "            \"‚ùå Error in Doctype Prediction\",\n",
        "            \"‚ùå Error in SBERT or Meta Lookup\",\n",
        "            \"‚ùå Error in FLAN Field Selection\",\n",
        "            \"‚ùå Error in FLAN QUERY Generation\"\n",
        "            f\"üí• Exception: {str(e)}\"\n",
        "        )\n",
        "iface = gr.Interface(\n",
        "    fn=full_pipeline,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"‚ùì Natural Language Question\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Markdown(label=\"Doctype Prediction\"),\n",
        "        gr.Markdown(label=\"Top Fields (SBERT)\"),\n",
        "        gr.Markdown(label=\"Selected Fields (FLAN)\"),\n",
        "        gr.Markdown(label=\"Frappe Query (FLAN)\")\n",
        "    ],\n",
        "    title=\"üîó Text2Frappe End-to-End Demo\",\n",
        "    description=\"This pipeline integrates Stage 1 (RoBERTa), Stage 2 Hybrid (SBERT + FLAN), and Stage 3 (FLAN-T5) to convert natural language questions into ERPNext queries.\"\n",
        ")\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "UlomK70dNcPy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}