{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1t_suUR3REOJml_PZ1Zy6fJC_vRL7xJpX",
      "authorship_tag": "ABX9TyO456j0acTzw01kHtUcEn7g"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO8gM7cQ8Q0R"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, losses, InputExample, models, LoggingHandler\n",
        "from torch.utils.data import DataLoader\n",
        "import logging\n",
        "import json"
      ],
      "metadata": {
        "id": "O9Uv8I_WHABK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "NMHIwQR4-2uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
        "                    level=logging.INFO,\n",
        "                    handlers=[LoggingHandler()])"
      ],
      "metadata": {
        "id": "5mlDup4D-5xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Changai/S2/S2 Datasets/S2_sbert_new.json\") as f:\n",
        "    triplet_data = json.load(f)"
      ],
      "metadata": {
        "id": "BTKtadVS-7Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "triplet_data"
      ],
      "metadata": {
        "id": "wpXlamedLxW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import InputExample\n",
        "\n",
        "train_examples = []\n",
        "for item in triplet_data:\n",
        "    anchor = item['anchor']\n",
        "    positive = item['positive']\n",
        "    for negative in item['negatives']:\n",
        "        train_examples.append(InputExample(texts=[anchor, positive, negative]))\n"
      ],
      "metadata": {
        "id": "HqhEHIseL5_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples"
      ],
      "metadata": {
        "id": "S07_eNlLYCBt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader=DataLoader(train_examples,shuffle=True,batch_size=16)"
      ],
      "metadata": {
        "id": "xYqryLs9YYiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "5G9l3QwLYnKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_Loss=losses.TripletLoss(model=model)"
      ],
      "metadata": {
        "id": "ZRU6D95nYr50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = '/content/drive/MyDrive/Changai/S2/S2 Model/flan_field_selector'"
      ],
      "metadata": {
        "id": "RLN4XgUhZt1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_objectives=[(train_dataloader,train_Loss)],\n",
        "          epochs=3,\n",
        "          warmup_steps=32,\n",
        "          output_path=output_path\n",
        "          )"
      ],
      "metadata": {
        "id": "TQKpUCquZzkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_doctype(text):\n",
        "    for line in text.split(\"\\n\"):\n",
        "        if line.lower().startswith(\"doctype:\"):\n",
        "            return line.split(\":\", 1)[1].strip()\n",
        "    return None"
      ],
      "metadata": {
        "id": "FqdY7_VRcmqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fields_for_doctype(meta: dict, doctype: str):\n",
        "    if doctype not in meta:\n",
        "        raise ValueError(f\"‚ùå Doctype '{doctype}' not found in metadata.\")\n",
        "    fields_dict = meta[doctype][\"fields\"]\n",
        "    return [f\"{key}: {desc}\" for key, desc in fields_dict.items()]"
      ],
      "metadata": {
        "id": "3vnUEAv2ipYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Changai/meta.json\") as f:\n",
        "  meta=json.load(f)"
      ],
      "metadata": {
        "id": "tWkxoXrAdoOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta"
      ],
      "metadata": {
        "id": "rk4l4hYeeEGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "B8GAYkzbGInf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "create_repo(\"text2frappe-s2-sbert\", private=True)"
      ],
      "metadata": {
        "id": "7DSvFPUrF_dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_folder\n",
        "\n",
        "upload_folder(\n",
        "    repo_id=\"hyrinmansoor/text2frappe-s2-sbert\",\n",
        "    folder_path=\"/content/drive/MyDrive/Changai/S2/S2 Model/sbert_topfield_selector\",\n",
        "    path_in_repo=\".\",  # root of the model repo\n",
        "    repo_type=\"model\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "7VRkNCU4GWNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîó Load Fine-Tuned SBERT Model from Hugging Face Hub (Stage 2 - Doctype Classification)\n"
      ],
      "metadata": {
        "id": "eM7HiDXGCljV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"hyrinmansoor/text2frappe-s2-sbert\"  # can be swapped anytime\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Now you can call through API Inference also.\n",
        "#API_URL = 'https://huggingface.co/hyrinmansoor/text2frappe-s2-sbert'"
      ],
      "metadata": {
        "id": "ZcxAXup0Gu43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SBERT-Based Top Field Selector Evaluation (Stage 2 - Field Ranking by Semantic Similarity)\n",
        "\n"
      ],
      "metadata": {
        "id": "lv5g8llXDBEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "def extract_doctype(query):\n",
        "    for line in query.split(\"\\n\"):\n",
        "        if line.lower().startswith(\"doctype:\"):\n",
        "            return line.split(\":\", 1)[1].strip()\n",
        "    return None\n",
        "\n",
        "def get_fields_for_doctype(meta, doctype):\n",
        "    if doctype not in meta:\n",
        "        raise ValueError(f\"‚ùå Doctype '{doctype}' not found in metadata.\")\n",
        "    fields_data = meta[doctype].get(\"fields\")\n",
        "    return fields_data\n",
        "test_cases=[\n",
        "    (\n",
        "        \"Doctype: Purchase Invoice Advance\\nQuestion: How much of the advance was allocated for PINV-00942?\",\n",
        "        [\"allocated_amount\"]\n",
        "    ),\n",
        "    (\n",
        "        \"Doctype: Purchase Invoice Advance\\nQuestion: What exchange rate was applied to document PINV-0452?\",\n",
        "        [\"ref_exchange_rate\"]\n",
        "    ),\n",
        "    (\n",
        "        \"Doctype: Closing Stock Balance\\nQuestion: Check whether the stock balance entry created on '2024-04-01' is still in Draft.\",\n",
        "        [\"status\"]\n",
        "    ),\n",
        "    (\n",
        "        \"Doctype: Sales Invoice Advance\\nQuestion: Find the exchange loss on any entries with ref exchange rate below 3.5.\",\n",
        "        [\"exchange_gain_loss\"]\n",
        "    ),\n",
        "    (\n",
        "        \"Doctype: Sales Invoice Advance\\nQuestion: What portion of advance has been allocated in entry SINVADV-2217?\",\n",
        "        [\"allocated_amount\"]\n",
        "    )\n",
        "]\n",
        "\n",
        "model = SentenceTransformer(\"/content/drive/MyDrive/Changai/S2/S2 Model/sbert_topfield_selector\")\n",
        "\n",
        "for i, (query, expected_fields) in enumerate(test_cases):\n",
        "    doctype = extract_doctype(query)\n",
        "    print(doctype)\n",
        "    if doctype is None:\n",
        "        print(f\"\\nTest Case {i+1}: Skipping - Could not extract doctype from query.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        fields = get_fields_for_doctype(meta, doctype)\n",
        "    except ValueError as e:\n",
        "        print(f\"\\nTest Case {i+1}: Skipping - {e}\")\n",
        "        continue\n",
        "\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "    field_embeddings = model.encode(fields, convert_to_tensor=True)\n",
        "    scores = util.cos_sim(query_embedding, field_embeddings)\n",
        "    k = min(len(fields), 5)\n",
        "    top_k = torch.topk(scores, k=len(fields))\n",
        "\n",
        "    print(f\"\\nTest Case {i+1}\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    print(f\"Extracted Doctype: {doctype}\")\n",
        "    print(\"Ranked Fields (Top 5):\")\n",
        "\n",
        "    for idx, score in zip(top_k.indices[0][:5], top_k.values[0][:5]):\n",
        "        field_name = fields[idx]\n",
        "        mark = \"‚úÖ\" if field_name in expected_fields else \"‚ùå\"\n",
        "        print(f\"{mark} {field_name} (score: {score:.4f})\")\n",
        "\n",
        "    top5_fields = [fields[idx] for idx in top_k.indices[0][:5]]\n",
        "    missed = [f for f in expected_fields if f not in top5_fields]\n",
        "    if missed:\n",
        "        print(\"‚ùóExpected fields NOT in top 5:\", missed)\n",
        "    else:\n",
        "        print(\"üéâ All expected fields are in the top 5!\\n\")"
      ],
      "metadata": {
        "id": "JFluGJc0cH52"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}