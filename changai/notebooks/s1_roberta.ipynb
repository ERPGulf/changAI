{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ERPGulf/changai/blob/hyrin/changai/notebooks/s1_roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp39mXFvd1Jf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iMSdtj0_xyQM"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('/content/drive/MyDrive/Changai/S1/Datasets/roberto_s1.json') as f:\n",
        "    raw_data = json.load(f)"
      ],
      "metadata": {
        "id": "yJMBm3sMMw_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKenV_vpzuc-"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "raw_data = Dataset.from_list(raw_data)\n",
        "split_dataset = raw_data.train_test_split(test_size=0.2, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svGQzZdN2dXd"
      },
      "outputs": [],
      "source": [
        "train_dataset = split_dataset['train']\n",
        "val_dataset = split_dataset['test']\n",
        "\n",
        "print(\"Train size:\", len(train_dataset))\n",
        "print(\"Validation size:\", len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6GOluyl2i0P"
      },
      "outputs": [],
      "source": [
        "# Save train\n",
        "train_dataset.to_json('/content/sample_data/train_data.json')\n",
        "\n",
        "# Save validation\n",
        "val_dataset.to_json('/content/sample_data/val_data.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQtoinKi8lBt"
      },
      "outputs": [],
      "source": [
        "# Remove 'instruction' from both datasets\n",
        "train_dataset = train_dataset.remove_columns(['instruction'])\n",
        "val_dataset = val_dataset.remove_columns(['instruction'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJaXeJfs8ow_"
      },
      "outputs": [],
      "source": [
        "print(train_dataset.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVo09k_J-EQu"
      },
      "outputs": [],
      "source": [
        "doctypes = sorted(list(set([record['output'] for record in raw_data])))\n",
        "\n",
        "label2id = {label: idx for idx, label in enumerate(doctypes)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "def encode_labels(example):\n",
        "    example['label'] = label2id[example['output']]\n",
        "    return example\n",
        "\n",
        "train_dataset = train_dataset.map(encode_labels)\n",
        "val_dataset = val_dataset.map(encode_labels)\n",
        "print(doctypes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Save to CSV format\n",
        "with open(\"doctype_mapping.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Doctype\", \"ID\"])  # Header\n",
        "    for label, idx in label2id.items():\n",
        "        writer.writerow([label, idx])\n",
        "\n",
        "print(\"Saved as doctype_mapping.csv\")"
      ],
      "metadata": {
        "id": "J4EIWkfgQke3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l7dGlloDlSp"
      },
      "outputs": [],
      "source": [
        "# As for now we are not working on the multi doctypes questions, roberto can be trained on that case later to predict multi labels.\n",
        "def is_single_doctype(example):\n",
        "    output = example['output']\n",
        "    # If output is a list (bad), or output contains ',' or ' and '\n",
        "    if isinstance(output, list):\n",
        "        return False\n",
        "    if ',' in output or ' and ' in output.lower():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Filter train and val datasets\n",
        "train_dataset = train_dataset.filter(is_single_doctype)\n",
        "val_dataset = val_dataset.filter(is_single_doctype)\n",
        "\n",
        "# Check sizes after cleaning\n",
        "print(\"Train size after cleaning:\", len(train_dataset))\n",
        "print(\"Validation size after cleaning:\", len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNNbYIHG-8sp"
      },
      "outputs": [],
      "source": [
        "train_dataset[4:9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ1XcJcO-sWT"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['input'], truncation=True, padding=\"max_length\")\n",
        "\n",
        "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_val = val_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX1aw_vzK7Zd"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaForSequenceClassification\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    'roberta-base',\n",
        "    num_labels=len(label2id),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QM3lyTxfKlcI"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaForSequenceClassification\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    'roberta-base',\n",
        "    num_labels=len(label2id),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlNKnTCIPluK"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2ul8PMTYsqE"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Changai/S1/Model\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ0ayoCxNGb-"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/Changai/S1/Model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Changai/S1/Model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "1xcIvxh8Bcpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "create_repo(\"text2frappe-s1\", private=True)"
      ],
      "metadata": {
        "id": "k3Njt-tZBoUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_folder\n",
        "\n",
        "upload_folder(\n",
        "    repo_id=\"hyrinmansoor/text2frappe-s1\",\n",
        "    folder_path=\"/content/drive/MyDrive/Changai/S1/Model\",\n",
        "    path_in_repo=\".\",  # root of the model repo\n",
        "    repo_type=\"model\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "RkFyDmB6Bx49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"hyrinmansoor/text2frappe-s1\"  # can be swapped anytime\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Now you can call through API Inference also.\n",
        "#API_URL = \"https://api-inference.huggingface.co/models/your-username/text2frappe-s1\""
      ],
      "metadata": {
        "id": "UocKwFIyCcFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdV73PGwdqW8"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n",
        "# Change the model_path to the directory where the model was actually saved\n",
        "model_path = \"/content/drive/MyDrive/Changai/S1/Model\"\n",
        "\n",
        "# Add local_files_only=True to explicitly load from the local path\n",
        "model = RobertaForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(model_path, local_files_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fERPlbaHkrS0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "test_data = [\n",
        "    {\"question\": \"Where can I view all active sales invoices?\", \"real_answer\": \"Sales Invoice\"},\n",
        "    {\"question\": \"How do I create a new quotation for a customer?\", \"real_answer\": \"Quotation\"},\n",
        "    {\"question\": \"List all delivery notes that are pending submission.\", \"real_answer\": \"Delivery Note\"},\n",
        "    {\"question\": \"Fetch all journal entries created last month.\", \"real_answer\": \"Journal Entry\"},\n",
        "    {\"question\": \"Show me approved purchase receipts from supplier ABC.\", \"real_answer\": \"Purchase Receipt\"},\n",
        "    {\"question\": \"Where can I check employee attendance records?\", \"real_answer\": \"Attendance\"},\n",
        "    {\"question\": \"Retrieve all leave applications pending approval.\", \"real_answer\": \"Leave Application\"},\n",
        "    {\"question\": \"Find stock entries made for production today.\", \"real_answer\": \"Stock Entry\"},\n",
        "    {\"question\": \"Display open support tickets for customers.\", \"real_answer\": \"Issue\"},\n",
        "    {\"question\": \"Where can I configure default buying policies?\", \"real_answer\": \"Buying Settings\"},\n",
        "    {\"question\": \"List all active customer subscriptions.\", \"real_answer\": \"Subscription\"},\n",
        "    {\"question\": \"Show all approved supplier quotations.\", \"real_answer\": \"Supplier Quotation\"},\n",
        "    {\"question\": \"Fetch submitted payment entries from this week.\", \"real_answer\": \"Payment Entry\"},\n",
        "    {\"question\": \"How to track production planning orders?\", \"real_answer\": \"Production Plan\"},\n",
        "    {\"question\": \"View all assets that are under maintenance.\", \"real_answer\": \"Asset Maintenance\"},\n",
        "]\n",
        "\n",
        "id2label = {str(k): v for k, v in id2label.items()}\n",
        "\n",
        "results = []\n",
        "\n",
        "for record in test_data:\n",
        "    test_question = record[\"question\"]\n",
        "    real_answer = record[\"real_answer\"]\n",
        "\n",
        "    inputs = tokenizer(test_question, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_class_id = logits.argmax(dim=-1).item()\n",
        "\n",
        "    predicted_doctype = id2label[str(predicted_class_id)]\n",
        "\n",
        "    # Store\n",
        "    results.append({\n",
        "        \"Question\": test_question,\n",
        "        \"Real Answer\": real_answer,\n",
        "        \"Model Prediction\": predicted_doctype,\n",
        "        \"Correct?\": \"✅\" if predicted_doctype == real_answer else \"❌\"\n",
        "    })\n",
        "df_results = pd.DataFrame(results)\n",
        "print(df_results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1elraxN0SpPcHuvAayU8_BusQqm9INN5d",
      "authorship_tag": "ABX9TyN6Fu1dAdYVISb7yVMg0tzB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}